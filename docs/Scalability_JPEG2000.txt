/**   Copyright 2014 Paul Roit
*
*   Licensed under the Apache License, Version 2.0 (the "License");
*   you may not use this file except in compliance with the License.
*   You may obtain a copy of the License at
*
*       http://www.apache.org/licenses/LICENSE-2.0
*
*   Unless required by applicable law or agreed to in writing, software
*   distributed under the License is distributed on an "AS IS" BASIS,
*   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
*   See the License for the specific language governing permissions and
*   limitations under the License.
*/

# Different types of progressiveness and scalability in JPEG2000

The goal of this article is to explain the differences in the four types 
of progressiveness in a JPEG2000 image. The types are:
Spatial location, Color component, Quality layer and Resolution level.
These types are directly related to the compression steps and the way
coded data is packed into packets.
I will not cover here spatial and color progressions, spatial
progression will be discussed under random spatial access, and color progression
is not very sophisticated once you understand how JPEG2000 treats color channels.

So lets dive deeper into resolution and quality scalability.

## What is a resolution level?

![Resolution Level](https://github.com/plroit/Skyreach/blob/master/docs/Resolution_Level.png)

Consider a source image of a given size WxH pixels. Sometimes we need to fit 
the image, or a part of the image, inside a viewport (a container window) that 
is smaller in terms of pixel area, than the desired part of the image.
What do we do? We zoom-out. In other terms, we scale-down the image, or
down-sample it, in such a way that it occupies less space at the expense of the
level of detail that becomes coarser. 

There are many methods to down-sample an image, and the best method to use
really depends on the visual content of the image and the visual system that 
will consume this image, may it be human or machine.
All of the methods try to approximate an ideal operation in the frequency domain:
Try to attenuate to zero all frequencies above a certain threshold, and 
reconstruct the image using the leftover low frequencies.

## Scale factor
Scale, or zoom factor, is just a scaling factor that the source image was scaled with.
The resolution of the resulting image is W\*scale x H\*scale pixels. 
Scale is a strictly positive real number. 
A scale factor of 1.0 means no scaling at all, while scaling by a factor greater 
than 1.0 means up-sampling and by a factor less than 1.0 is down-sampling.

## Image Pyramid

![Image Pyramid](https://github.com/plroit/Skyreach/blob/master/docs/Image_Pyramid.png)

Lets pick a series of scaling factors that form a geometric series, 
for example: 1/2, 1/4, 1/8, 1/16, 1/32.. and create a set of scaled down images
originating at the source image. Each image will be a quarter of the size of
the previous image. This is an image pyramid!

An image pyramid is useful as a pre-processing technique. Once viewing a 
scaled-down version of the image becomes too computationally intensive, 
or happens often enough, it is wise to trade the computational cost with 
a memory cost. 
If a user frequents a scaled-down image often, it is wiser to store the 
scaled-down image for the next access attempt instead of recomputing it. 
All the more wiser when the image is of very high resolution to begin with,
and creating a scaled version takes a considerable amount of computational
resources at the first access attempt.

## Image pyramid in JPEG2000

![JPEG2000 Wavelet Decomposition](https://github.com/plroit/Skyreach/blob/master/docs/JPEG2000_Wavelet_Decomposition.png)

The JPEG2000 compression scheme uses a technique called wavelet decomposition
to create a representation of an image pyramid. Wavelet decomposition is a 
broad subject for academic research, and JPEG2000 is not the first compression
algorithm to employ it. The decomposition by itself does not compress data,
but it is used to de-correlate high frequency signals in the image from the low,
for a more efficient compression by other steps in the encoder.

The decomposition is a recursive process. At each step, an input image 
is divided into four images, each a quarter of the size of the input.
Each of the four images is called a subband. One subband that is labeled 'LL' 
is actually a scaled-down version of the input, with a scaling factor 
of a 1/2. The recursion continues with the 'LL' subband now being the input for
the next decomposition step. At each successive level r, the resulting 
subbands, and the latest 'LL' band among them, are of scale 1/(2^r) of the 
original image. The number of decomposition steps is defined by 
the encoder.

Take for example an image that was decomposed using R recursive
steps. The image content is transformed to R*3 residual subbands
and the remaining LL subband. That subband represents the smallest scaled-down
version of the image, or a 1/(2^R) of the original image. During
the recursive decomposition the encoder encountered R different, scaled-down
versions of the original image, each of size 1/(2^r) where r is in the 
inclusive range: [1, R]. 

Because of the nature of the decomposition, the subbands are not
a straight forward image pyramid. To reconstruct a scaled-down version of the 
image at scale factor 1/(2^r), one needs to decode all of the subbands starting 
from the final 'LL' and up through the required level, then apply an inverse 
wavelet decomposition, reconstructing every upper 'LL' subband from the four 
lower subbands, until the required resolution level is reached. This may seem 
computationally complex and counter the benefits of an image pyramid. One 
should however note that access to the lower resolution levels in the pyramid 
is very efficient and that the computational cost rises with the number of required
pixels in the target resolution level. It should also be noted that in any event,
data from resolutions higher than the target level is unnecessary to the 
reconstruction of the image at the required level.

## Progressiveness via resolution level

After this long introduction it is now fairly easy to describe what is a 
resolution progressiveness: The more bits a decoder reads from the coded image,
the higher the resolution level it can extract. The first bits will contain the
final 'LL' subband. Successive bits will contain more and more subbands, thus 
leading to finer details being revealed at successive resolution levels.

## Scalability via low resolution up-sampling

The following images are taken from the Dresden Image Database
https://forensics.inf.tu-dresden.de/ddimgdb

Lets take a look at the original image, lossless compressed:
![Fine branches](https://github.com/plroit/Skyreach/blob/master/docs/Nikon_cropped.png)
Did you notice the fine tips of the tree branches?

Sophisticated image viewing applications today provide efficient and scalable
methods to view images. These applications try to minimize the time and resources
it takes to provide a meaningful image to the end-user, even under constraining
conditions. Even if the image data did not finish loading, an image viewer
will try to use the available data to make a partial reconstruction of the image,
and display that to the user. 

![Up-sampled](https://github.com/plroit/Skyreach/blob/master/docs/upsampled_image_from_R3.png)
Can you notice the branches now? 

For low bandwidth clients, a partial reconstruction enhances user experience,
and gives a feel of fluency till the entire image is loaded. A common technique 
is to deliver first a low resolution subset of the image, and then to up-sample
it. Simply put, use the pixels that cover a low resolution area to create an
image of higher resolution that resemble good enough the actual high resolution 
data.It is easier to load a low resolution subset of the image since it takes
less pixels and therefore less bits.
the user gets a partial response faster and the application continues to deliver
the required high resolution content and refine the produced image, fluently.
This technique is very common, and is very practical with image systems that use 
resolution pyramids either as external images or internal within the compressed
content of the original image, as in JPEG2000 image format.

## Up-sampling is not magic

It is important to note that up-sampling is not magic. Up-sampling an image 
will not produce fine details that were only visible in the high frequency 
components of the image. It can only be a replacement for a short period of 
time, till the user will get the true information. 

To achieve a more fluent transition between the up-sampled image and the true
high resolution image, we need to choose an up-sampling method that best suits
to the human visual system and the content of the image. As in down-sampling, 
there is a multitude of methods to choose from. There is an extensive overview 
of different methods at: http://www.imagemagick.org/Usage/filter and the bottom
line of the review is to use a Mitchell filter for up-sampling: 
http://www.imagemagick.org/Usage/filter/#best_filter

## Quality scalability in JPEG2000

JPEG2000 compressed images can be set to have multiple quality increments of 
the image. These quality increments are spread over any resolution level and 
spatial location. A partial quality is obtained by  setting truncation points
during encoding. The truncation points are set inside the coded data at every 
resolution, color component and spatial location. The number of truncation 
points is the number of available quality increments, it can be very large, and 
is not limited by, or related to, to the number of resolution levels.
Decoding a partial quality image is done simply by reconstructing an image from
data that is read up to a truncation point. 

A partial quality image is **not** obtained by up-sampling a low resolution 
image. A partial quality image will have data taken from all resolution levels.
Any resolution level can be reconstructed with partial quality. 

To understand the differences between resolution up-sampling and quality layers
I have put together a series of 4 up-sampled images from different resolutions,
and a series of images with different bit-rates. Each resolution level in 
the first series occupies 4 times less area than the previous image. The levels
are obtained by decoding the original JPEG2000 image.

Each quality layer image has 4 times less bytes than the previous one. This 
setting is chosen in advance, and an encoder can choose almost any arbitrary 
value. The number of possible quality layers can be 1 to 2^16, though for 
practical reasons encoders limit this range to 1 to several dozens.
The original image has been compressed visually lossless by JPEG2000 encoder.
Quality layers are formed when the image is encoded. The layers in the following
image are specifically set to have their bit-rates a geometric series with a 
factor of 0.25. 

bpp = bits per pixel, or bit-rate measurement.

Please note that it is **not** a well defined comparison, but it gives a good 
hunch.

![R4 vs L1](https://github.com/plroit/Skyreach/blob/master/docs/upsampling_vs_quality_R4_vs_L1.png)

![R4 vs L1](https://github.com/plroit/Skyreach/blob/master/docs/upsampling_vs_quality_R4_vs_L1.png)

![R4 vs L1](https://github.com/plroit/Skyreach/blob/master/docs/upsampling_vs_quality_R4_vs_L1.png)

![R4 vs L1](https://github.com/plroit/Skyreach/blob/master/docs/upsampling_vs_quality_R4_vs_L1.png)

Lets compare the byte sizes of the images according to their reduction factor.
The size of the resolution level in JPEG2000 is obtained by using a transcoding 
application that can write a new jp2 image out of an existing one by discarding 
packets that belong to higher resolution levels.

|	Reduction Factor	|	Bit-rate by resolution	| Bit-rate by quality	|
|:---------------------:|:-------------------------:|:---------------------:|
|0 - original			|4.8						|4.8					|
|4						|10.6						|1.19					|
|16						|18.2						|0.29					|
|64						|25.0						|0.078					|
|256					|31.4						|0.018					|

You can actually witness an odd anomaly here of JPEG2000, a lower resolution
takes more bits to compress per pixel. This is odd.
It does not mean that smaller images are harder to compress. It means that more
bits are used per pixel in the lower resolution levels than in the higher.

Can I fully explain this phenomenon? Not at this time. 
However I will try to reconstruct a low resolution level using only a subset of
quality layers. Then I will compare the PSNR difference of the image to the above
low resolution reference image. 
I will use the  transcoding application again, this time obtaining a new jp2 
image from an existing one by discarding packets that belong to a high 
resolution and high quality layers.
At each reduction factor I will obtain a new image from reduced resolution
(reduction by pixel area) and reduced quality layer (reduction by bit-rate).
Please note that the quality layer bit-rates were set globally for the entire 
image, and the quality layers at each resolution may have a different 
distribution than the global bit-rates.

PSNR - a measure of similarity between images - higher is better
http://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio


|Reduction Factor|bpp of reduced resolution|bpp of reduced resolution and quality| PSNR|
|:-------------:|:-------------------------:|:----------------------------------:|:---:|
|4				|10.6						|4.42								|43.0
|16				|18.2 						|3.61								|35.3
|64				|25.0						|3.05								|30.0
|256			|31.4						|3.12								|24.7

Can you spot the difference?

![Resolution vs Quality-Resolution Reduction](https://github.com/plroit/Skyreach/blob/master/docs/JPEG2000_Quality_Resolution_VS_Resolution_Reduction.png)
___
##### 60 seconds about quality layer formation - skip if too misleading.

Please note that the above explanation is a little superficial. For more in-depth
understanding consult further articles about the encoding process.
See EBCOT algorithm - Embedded Block Coding with Optimal Truncation.
A codeblock is a small rectangle of samples in every subband and tile-component.
Once all are encoded, a rate allocator calculates the best truncation points 
for every codeblock according to some metric. The job of the rate allocator is
to divide the codeblock data to different buckets. Each bucket contains an 
increase in visual quality from the previous bucket. The number of buckets is 
defined globally by the encoder.
** Basically, a bucket is a layer of image quality.** 

Some codeblocks will contribute data to all buckets. Some will contribute only 
to the first buckets. A good rate allocator will try to optimize the division
such that each bucket will have a descending order of ratios between
the visual quality metric to the cost - which is the number
of bytes added by the bucket. The descending order means that buckets are 
ordered by their cost-effectiveness. 
A rate-allocator can set a final target amount of bytes for each bucket, 
or it can set a target PSNR (peak signal to noise ratio) value for each bucket.
Other targets for each bucket are possible.

##### End of 60 seconds
___
  




